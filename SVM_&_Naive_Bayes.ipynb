{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It is especially well known for its performance in binary classification problems.\n",
        "\n",
        "How It Works (Intuitively):\n",
        "\n",
        "SVM finds the best decision boundary (called a hyperplane) that separates classes in the feature space with the largest possible margin.\n",
        "\n",
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "Support Vector Machines (SVMs) aim to find the optimal separating hyperplane between classes. The key difference between Hard Margin and Soft Margin lies in how strictly they separate the data.\n",
        "\n",
        "Hard Margin SVM\n",
        "\n",
        "Assumes that the data is perfectly linearly separable.\n",
        "No misclassifications are allowed — all points must be on the correct side of the margin.\n",
        "\n",
        "Soft Margin SVM\n",
        "\n",
        "Allows some misclassifications or margin violations.\n",
        "\n",
        "Introduces a penalty parameter C to control the trade-off between maximizing the margin and minimizing classification error.\n",
        "\n",
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "\n",
        "The Kernel Trick is a technique used in Support Vector Machines (SVMs) that allows them to implicitly map input data into a higher-dimensional feature space where a linear decision boundary can be found, even if the data is not linearly separable in its original space. This is achieved without explicitly calculating the coordinates of the data in the higher-dimensional space, which saves significant computational cost. Instead, a kernel function computes the dot product of the transformed data points in the higher-dimensional space directly from the original lower-dimensional data.\n",
        "\n",
        "One example of a kernel is the Radial Basis Function (RBF) Kernel, also known as the Gaussian Kernel.\n",
        "RBF Kernel Use Case:\n",
        "The RBF kernel is widely used for non-linear classification problems where the decision boundary is complex and cannot be represented by a straight line or plane in the original feature space. For instance, consider a dataset where two classes are intertwined in a circular pattern in a 2D plane. A linear SVM would fail to separate these classes. The RBF kernel implicitly projects these 2D points into a higher-dimensional space (e.g., 3D), where a hyperplane can effectively separate the classes. This allows the SVM to find a non-linear decision boundary in the original 2D space, such as a circle or an ellipse, which accurately separates the data. It is particularly effective when dealing with data that exhibits complex, non-linear relationships between features.\n",
        "\n",
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "A Naive Bayes classifier is a probabilistic machine learning algorithm that uses Bayes' theorem to predict the probability of a data point belonging to a specific class. It's called \"naïve\" because it makes the simplifying assumption that all features used to make the prediction are independent of each other, which is often not true in real-world scenarios. Despite this simplification, Naive Bayes can be surprisingly effective, especially in text classification tasks.\n",
        "\n",
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "\n",
        "Naive Bayes classifiers come in different flavors, each suited for different types of data. Gaussian Naive Bayes is used for continuous data, assuming a normal distribution. Multinomial Naive Bayes is used for discrete data, often word counts in text. Bernoulli Naive Bayes is used for binary or boolean features.\n",
        "\n"
      ],
      "metadata": {
        "id": "YGJAe73fADdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''You can use any suitable datasets like Iris, Breast Cancer, or Wine from\n",
        "sklearn.datasets or a CSV file you have.\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\n",
        "(Include your Python code and output in the code box below.)'''\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an SVM Classifier with linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\" Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Print support vectors\n",
        "print(\"\\n Support Vectors:\")\n",
        "print(svm_model.support_vectors_)\n",
        "\n",
        "# Print support vector indices per class\n",
        "print(\"\\n Support Vector Indices per Class:\")\n",
        "print(svm_model.support_)\n",
        "\n",
        "# Print number of support vectors per class\n",
        "print(\"\\nNumber of Support Vectors per Class:\")\n",
        "print(svm_model.n_support_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7HZ1lqcB8fd",
        "outputId": "7d9ac8d6-0cfa-4af1-981c-9e2faa6e3727"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model Accuracy: 1.00\n",
            "\n",
            " Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n",
            "\n",
            " Support Vector Indices per Class:\n",
            "[ 31  33  91  22  45  54  59  60  62  73  79  80 105 110   5  16  30  42\n",
            "  68  81  87 101 112 113 116]\n",
            "\n",
            " Number of Support Vectors per Class:\n",
            "[ 3 11 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 7: Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\n",
        "'''\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into train and test sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Gaussian Naive Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print accuracy\n",
        "print(f\" Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\n Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5pNyEGwB80d",
        "outputId": "3e70d556-6f0f-42b3-b35c-c8c70b0416ce"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Accuracy: 0.97\n",
            "\n",
            " Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 8: Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy'''\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define SVM and parameter grid for GridSearch\n",
        "svm = SVC(kernel='rbf')\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': ['scale', 0.001, 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best estimator and predictions\n",
        "best_svm = grid_search.best_estimator_\n",
        "y_pred = best_svm.predict(X_test)\n",
        "\n",
        "# Output results\n",
        "print(\" Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(f\"Accuracy on test set: {accuracy_score(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZrFhEuVB828",
        "outputId": "21874699-744a-4899-dba4-e138a11d2e3b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Best Hyperparameters: {'C': 100, 'gamma': 'scale'}\n",
            "Accuracy on test set: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 9: Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions'''\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load subset of 20 newsgroups (binary classification)\n",
        "categories = ['comp.graphics', 'sci.med']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X = newsgroups.data\n",
        "y = newsgroups.target  # 0 or 1\n",
        "\n",
        "# Vectorize text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_tfidf = vectorizer.fit_transform(X)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Multinomial Naive Bayes\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for positive class\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "print(f\" ROC-AUC score: {roc_auc:.3f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4_o1Cf9B86R",
        "outputId": "20fae3c3-f679-4c5d-80c0-e9c5066bdb5b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ROC-AUC score: 0.989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "'''\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from sklearn.utils import compute_sample_weight\n",
        "\n",
        "# Load a subset of 20 newsgroups as proxy for spam (sci.crypt) and ham (talk.politics.misc)\n",
        "categories = ['sci.crypt', 'talk.politics.misc']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "X_raw = data.data\n",
        "y = data.target  # 0 or 1\n",
        "\n",
        "# Simulate some missing data: replace 5% of samples with empty strings\n",
        "rng = np.random.default_rng(42)\n",
        "missing_indices = rng.choice(len(X_raw), size=int(0.05 * len(X_raw)), replace=False)\n",
        "for idx in missing_indices:\n",
        "    X_raw[idx] = \"\"\n",
        "\n",
        "# Text vectorization with TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X = vectorizer.fit_transform(X_raw)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Handle class imbalance with sample weights (Naive Bayes doesn't have class_weight param)\n",
        "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "\n",
        "# Train Multinomial Naive Bayes\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train, sample_weight=sample_weights)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIYFXmF3B9Lr",
        "outputId": "9d63c687-c619-4ac8-944d-2bb32aef3e0d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "         sci.crypt       0.97      0.83      0.89       199\n",
            "talk.politics.misc       0.82      0.97      0.88       155\n",
            "\n",
            "          accuracy                           0.89       354\n",
            "         macro avg       0.89      0.90      0.89       354\n",
            "      weighted avg       0.90      0.89      0.89       354\n",
            "\n",
            "ROC-AUC Score: 0.976\n"
          ]
        }
      ]
    }
  ]
}
